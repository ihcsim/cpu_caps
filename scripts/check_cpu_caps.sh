#!/bin/bash

# This script collects the virsh capabilities files from all virt-handler pods in
# the ${KUBEVIRT_NAMESPACE} namespace.
#
# If the ${VIRT_LAUNCHER_IMAGE_CUSTOM} variable is specified, it starts an
# ephemeral container in each virt-handler pod using the custom virt-launcher
# image to collect the same set of virsh capabilities files. This allows us to
# compare the collected CPU capabilities identify by the different versions of
# virt-handler.
#
# The ephemeral container executes the built-in node-labeller.sh script, writes
# the output XML files to the container's /var/lib/kubevirt-node-labeller, and
# copies the output from the container to your shell.
#
# This script requires kubeconfig to be included in $PATH, with permissions to
# run `kubectl [debug|cp|exec]` targeting the ${KUBEVIRT_NAMESPACE} namespace.

set -e

KUBEVIRT_NAMESPACE=${KUBEVIRT_NAMESPACE:-harvester-system}
DEBUGGER_TTL_SECONDS=3600

now=$(date "+%Y%m%d-%H%M%S")
src_dir=$(dirname $(readlink -f -- "${BASH_SOURCE[0]}"))
virt_handlers=($(kubectl -n "${KUBEVIRT_NAMESPACE}" get po \
  -lkubevirt.io=virt-handler \
  --no-headers \
  -ojsonpath='{range .items[]}{.metadata.name},{.spec.nodeName}{"\n"}{end}'))

# collect the virsh capabilities .xml files from the hostpath
# /var/lib/kubevirt-node-labeller.
function cpu_caps() {
  local cluster_image=$(kubectl -n "${KUBEVIRT_NAMESPACE}" get ds virt-handler -ojsonpath='{.spec.template.spec.initContainers[?(@.name=="virt-launcher")].image}')
  echo "➡️ version: ${cluster_image}"

  for virt_handler in "${virt_handlers[@]}"; do
    local pod_name=$(echo "${virt_handler}" | cut -d',' -f1)
    local node_name=$(echo "${virt_handler}" | cut -d',' -f2)
    echo "  ☸ ${KUBEVIRT_NAMESPACE}/${pod_name}"

    local image_tag=$(echo "${cluster_image}" | cut -d":" -f2)
    kubectl -n "${KUBEVIRT_NAMESPACE}" cp -c virt-handler "${pod_name}":/var/lib/kubevirt-node-labeller/ ./out-"${now}/${node_name}/${image_tag}"
  done
}

# start an ephemeral container to collect the virsh capabilities .xml files
# generated by the custom virt-handler. the debugger references the in-container
# /var/lib/kubevirt-node-labeller folder, not the one on the host.
function cpu_caps_custom() {
  local custom_image="${VIRT_LAUNCHER_IMAGE_CUSTOM}"
  echo "➡️ version: ${custom_image}"

  for virt_handler in "${virt_handlers[@]}"; do
    local pod_name=$(echo "${virt_handler}" | cut -d',' -f1)
    local node_name=$(echo "${virt_handler}" | cut -d',' -f2)
    local debugger_name=debug-"${now}"
    echo "  ☸ ${KUBEVIRT_NAMESPACE}/${pod_name}"
    kubectl -n "${KUBEVIRT_NAMESPACE}" debug \
      --image="${custom_image}" \
      --container "${debugger_name}" \
      --profile=general \
      --custom="${src_dir}"/scc.yaml \
      "${pod_name}" -- /bin/bash -c "set -e; mkdir -p /var/lib/kubevirt-node-labeller; node-labeller.sh; touch /var/lib/kubevirt-node-labeller/.done; sleep ${DEBUGGER_TTL_SECONDS}"

    set +e
    echo "    ⚙️ waiting for debugger ${pod_name}/${debugger_name} to come online..."
    while true; do
      kubectl -n "${KUBEVIRT_NAMESPACE}" exec -c "${debugger_name}" "${pod_name}" -- ls -al /var/lib/kubevirt-node-labeller/.done >/dev/null 2>&1
      if [ "$?" -eq 0 ]; then
        break
      fi
      sleep 5
    done
    set -e

    local image_tag=$(echo "${custom_image}" | cut -d":" -f2)
    kubectl -n "${KUBEVIRT_NAMESPACE}" cp -c "${debugger_name}" "${pod_name}":/var/lib/kubevirt-node-labeller ./out-"${now}/${node_name}/${image_tag}"
  done
}

function tarball() {
  tar -czf ./out-"${now}".tar.gz ./out-"${now}"
  rm -rf ./out-"${now}"
}

echo "fetching virsh capabilities files..."
cpu_caps
if [ ! -z "${VIRT_LAUNCHER_IMAGE_CUSTOM}" ]; then
  cpu_caps_custom
fi
tarball
echo "output saved to ./out-${now}.tar.gz"
